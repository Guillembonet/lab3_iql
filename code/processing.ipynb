{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp: dict[str, spacy.Language] = {}\n",
    "\n",
    "nlp[\"en\"] = spacy.load(\"en_core_web_sm\")\n",
    "nlp[\"en\"].max_length = 2000000\n",
    "\n",
    "sources = {\n",
    "    \"en\": \"../data/moby_dick.txt\",\n",
    "}\n",
    "\n",
    "sources_data = {}\n",
    "for language in sources:\n",
    "    file = open(sources[language], 'r')\n",
    "    contents = file.read()\n",
    "    sources_data[language] = contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "218310\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text: str, model_lang: spacy.Language) -> list[str]:\n",
    "    doc = model_lang(text)\n",
    "    tokens = [token.text for token in doc if not token.is_space and not token.is_punct and not token.is_digit]\n",
    "    return tokens\n",
    "\n",
    "def tokens(sources_data: dict[str, str]) -> dict[str, list[str]]:\n",
    "    tokenized_sources = {}\n",
    "    for language in sources_data:\n",
    "      tokenized_sources[language] = tokenizer(sources_data[language], nlp[language])\n",
    "    return tokenized_sources\n",
    "\n",
    "tokenized_sources = tokens(sources_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50/218309\n",
      "Progress: 100/218309\n",
      "Progress: 150/218309\n",
      "Progress: 200/218309\n",
      "Progress: 250/218309\n",
      "Progress: 300/218309\n",
      "Progress: 350/218309\n",
      "Progress: 400/218309\n",
      "Progress: 450/218309\n",
      "Progress: 500/218309\n",
      "Progress: 550/218309\n",
      "Progress: 600/218309\n",
      "Progress: 650/218309\n",
      "Progress: 700/218309\n",
      "Progress: 750/218309\n",
      "Progress: 800/218309\n",
      "Progress: 850/218309\n",
      "Progress: 900/218309\n",
      "Progress: 950/218309\n",
      "Progress: 1000/218309\n",
      "Progress: 1050/218309\n",
      "Progress: 1100/218309\n",
      "Progress: 1150/218309\n",
      "Progress: 1200/218309\n",
      "Progress: 1250/218309\n",
      "Progress: 1300/218309\n",
      "Progress: 1350/218309\n",
      "Progress: 1400/218309\n",
      "Progress: 1450/218309\n",
      "Progress: 1500/218309\n",
      "Progress: 1550/218309\n",
      "Progress: 1600/218309\n",
      "Progress: 1650/218309\n",
      "Progress: 1700/218309\n",
      "Progress: 1750/218309\n",
      "Progress: 1800/218309\n",
      "Progress: 1850/218309\n",
      "Progress: 1900/218309\n",
      "Progress: 1950/218309\n",
      "Progress: 2000/218309\n",
      "Progress: 2050/218309\n",
      "Progress: 2100/218309\n",
      "Progress: 2150/218309\n",
      "Progress: 2200/218309\n",
      "Progress: 2250/218309\n",
      "Progress: 2300/218309\n",
      "Progress: 2350/218309\n",
      "Progress: 2400/218309\n",
      "Progress: 2450/218309\n",
      "Progress: 2500/218309\n",
      "Progress: 2550/218309\n",
      "Progress: 2600/218309\n",
      "Progress: 2650/218309\n",
      "Progress: 2700/218309\n",
      "Progress: 2750/218309\n",
      "Progress: 2800/218309\n",
      "Progress: 2850/218309\n",
      "Progress: 2900/218309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokenized_sources[language])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     47\u001b[0m     data_d\u001b[38;5;241m=\u001b[39m process_data_d(tokenized_sources[language], d)\n\u001b[0;32m---> 48\u001b[0m     I_d[d] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_I_d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m d \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokenized_sources[language])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m, in \u001b[0;36mcalculate_I_d\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     37\u001b[0m         p_y_d \u001b[38;5;241m=\u001b[39m f_y_d[token_y]\u001b[38;5;241m/\u001b[39mF\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# sum p(x,y|d)*log(p(x,y|d)/(p(x|d)*p(y|d))) for each x and y\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m         I_d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_x_y \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(p_x_y\u001b[38;5;241m/\u001b[39m(p_x_d \u001b[38;5;241m*\u001b[39m p_y_d))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m I_d\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def process_data_d(data: list[str], d: int) -> dict[str, dict[str, int]]:\n",
    "    # dict of token x -> token y -> f(x,y)\n",
    "    processed_data: dict[str, dict[str, int]] = {}\n",
    "    data_length = len(data)\n",
    "    for i in range(data_length):\n",
    "        token_data = processed_data.get(data[i], {})\n",
    "        # TODO: should x == y be excluded?\n",
    "        if i+d < data_length and data[i] != data[i+d]:\n",
    "            # add 1 to the frequency of the token y that appears d tokens after the current token x\n",
    "            token_data[data[i+d]] = token_data.get(data[i+d], 0) + 1\n",
    "        elif i+d >= data_length:\n",
    "            break\n",
    "        processed_data[data[i]] = token_data\n",
    "    return processed_data\n",
    "\n",
    "def calculate_I_d(data: dict[str, dict[str, int]]) -> float:\n",
    "    # calculate f(x|d) and f(y|d) and F\n",
    "    f_x_d: dict[str, int] = {}\n",
    "    f_y_d: dict[str, int] = {}\n",
    "    F = 0\n",
    "    for token_x in data:\n",
    "        f_x_d[token_x] = 0\n",
    "        for token_y in data[token_x]:\n",
    "            f_y_d[token_y] = f_y_d.get(token_y, 0) + data[token_x][token_y]\n",
    "            f_x_d[token_x] += data[token_x][token_y]\n",
    "            F += data[token_x][token_y]\n",
    "\n",
    "    # calculate I(d)\n",
    "    I_d = 0\n",
    "    for token_x in data:\n",
    "        for token_y in data[token_x]:\n",
    "            p_x_y = data[token_x][token_y]/F\n",
    "            p_x_d = f_x_d[token_x]/F\n",
    "            p_y_d = f_y_d[token_y]/F\n",
    "            # sum p(x,y|d)*log(p(x,y|d)/(p(x|d)*p(y|d))) for each x and y\n",
    "            I_d += p_x_y * math.log(p_x_y/(p_x_d * p_y_d))\n",
    "    return I_d\n",
    "\n",
    "for language in tokenized_sources:\n",
    "    I_d: dict[int, float] = {}\n",
    "    x = []\n",
    "    y = []\n",
    "    start = time.time()\n",
    "    for d in range(1, len(tokenized_sources[language])-1):\n",
    "        data_d= process_data_d(tokenized_sources[language], d)\n",
    "        I_d[d] = calculate_I_d(data_d)\n",
    "        if d%500 == 0 and d != 0:\n",
    "            end = time.time()\n",
    "            print(f\"Progress: {d}/{len(tokenized_sources[language])-1} (elapsed time: {end - start})\")\n",
    "            start = time.time()\n",
    "        x.append(d)\n",
    "        y.append(I_d[d])\n",
    "\n",
    "    print(f\"Language: {language}\")\n",
    "    print(f\"I(d) for d=1: {I_d[1]}\")\n",
    "    print(f\"I(d) for d=2: {I_d[2]}\")\n",
    "    print(f\"I(d) for d=3: {I_d[3]}\")\n",
    "    print(f\"I(d) for d=4: {I_d[4]}\")\n",
    "    print(f\"I(d) for d=5: {I_d[5]}\")\n",
    "    print()\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
